{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8eaf532-f95b-475d-9a3c-a43cdbf0bb9d",
   "metadata": {
    "id": "b8eaf532-f95b-475d-9a3c-a43cdbf0bb9d"
   },
   "source": [
    "### Build and train a simple Recurrent Neural Network\n",
    "We are going to use writings of shakespear as our data.\n",
    "\n",
    "Steps:\n",
    "- Download the data\n",
    "- Preprocessing the data\n",
    "- Create character tokenizer\n",
    "- Tokenize data\n",
    "- Split data (train / test)\n",
    "- Create a batches of data to be fed to the model\n",
    "- Define parameters for the model\n",
    "- Initialize the model\n",
    "- Overfit a single batch\n",
    "- Train the model\n",
    "- Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995f77ce-1d55-41e7-a672-03aa81e97231",
   "metadata": {
    "id": "995f77ce-1d55-41e7-a672-03aa81e97231"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wKsjXyYxh-M2",
   "metadata": {
    "id": "wKsjXyYxh-M2"
   },
   "outputs": [],
   "source": [
    "# device (cpu or gpu) depending on what is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d435ded-dedc-4589-8897-5b7e241f433a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d435ded-dedc-4589-8897-5b7e241f433a",
    "outputId": "06a3faa1-377e-47db-9e08-6aca7c4ee82b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-28 21:07:20--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "\r",
      "input.txt             0%[                    ]       0  --.-KB/s               \r",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2024-04-28 21:07:20 (32.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab2f339b-37c2-4667-9ed1-6af1f38ac1c0",
   "metadata": {
    "id": "ab2f339b-37c2-4667-9ed1-6af1f38ac1c0"
   },
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7297bac4-93be-4907-810e-f5c1f7169e92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7297bac4-93be-4907-810e-f5c1f7169e92",
    "outputId": "8d829617-f239-4d00-bc92-99310a738637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of characters: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26294ebc-d7a9-45f7-b119-de4d918040b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "26294ebc-d7a9-45f7-b119-de4d918040b6",
    "outputId": "9736ee37-aeea-4260-c369-bf11caf24768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 65\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'Number of unique characters: {vocab_size}')\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a276a0f-6696-4e69-b160-8439ea79187e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a276a0f-6696-4e69-b160-8439ea79187e",
    "outputId": "2dd5112c-7cb1-41f9-9cf6-dc7b9c1e033c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43, 2]\n",
      "hello there!\n"
     ]
    }
   ],
   "source": [
    "# Character level tokenizer\n",
    "stoi = {char: idx for idx, char in enumerate(chars)}\n",
    "itos = {idx: char for char, idx in stoi.items()}\n",
    "\n",
    "encode = lambda s: [stoi[char] for char in s]\n",
    "decode = lambda i: ''.join([itos[idx] for idx in i])\n",
    "\n",
    "print(encode('hello there!'))\n",
    "print(decode(encode('hello there!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65088e39-5829-4e24-b94d-0683fd62086f",
   "metadata": {
    "id": "65088e39-5829-4e24-b94d-0683fd62086f"
   },
   "outputs": [],
   "source": [
    "# Tokenize all text\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb208371-fc18-4e14-8001-332ba7e2045b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb208371-fc18-4e14-8001-332ba7e2045b",
    "outputId": "9ae256bd-bead-4d60-aff4-b24c4b424414"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looking at some raw text\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "005e0dc7-8934-49e9-a73a-c1052624b4b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "005e0dc7-8934-49e9-a73a-c1052624b4b1",
    "outputId": "61773a3f-cbee-4b80-9125-0ff5c8379a0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at tokenized text\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d43f06-4825-44ac-beaf-1b1839dce0f0",
   "metadata": {
    "id": "93d43f06-4825-44ac-beaf-1b1839dce0f0"
   },
   "outputs": [],
   "source": [
    "def train_test_split(data, test_pct=.1):\n",
    "    data_size = len(data)\n",
    "    num_test_samples = int(data_size * test_pct)\n",
    "    test_data = data[-num_test_samples:]\n",
    "    train_data = data[:data_size - num_test_samples]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61ec0be6-9bf7-4df4-8625-36470cf19c95",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61ec0be6-9bf7-4df4-8625-36470cf19c95",
    "outputId": "14c92c34-5304-4a9e-ef83-5ed0c27c2ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set: 1003855\n",
      "size of test set: 111539\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data)\n",
    "print(f'size of training set: {len(train_data)}')\n",
    "print(f'size of test set: {len(test_data)}')\n",
    "\n",
    "assert len(train_data) + len(test_data) == len(data), \"Lost data during train test split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69841c3d-c8f4-42a1-9573-cfe6230729ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69841c3d-c8f4-42a1-9573-cfe6230729ad",
    "outputId": "304eca88-8f81-4087-97fe-8c3995bb904f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:\n",
      "tensor([[45, 46, 58,  8,  0,  0, 35, 13],\n",
      "        [25, 53, 56, 43,  1, 58, 46, 39],\n",
      "        [43, 56,  1, 44, 53, 59, 52, 58],\n",
      "        [ 1, 57, 43, 52, 58,  1, 58, 53]])\n",
      "labels\n",
      "tensor([[46, 58,  8,  0,  0, 35, 13, 30],\n",
      "        [53, 56, 43,  1, 58, 46, 39, 52],\n",
      "        [56,  1, 44, 53, 59, 52, 58, 39],\n",
      "        [57, 43, 52, 58,  1, 58, 53,  1]])\n",
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "# Function to get a batch\n",
    "def get_random_batch(data, batch_size=4, block_size=8):\n",
    "    # Get  batch_size of random indices which will be the start of sequence\n",
    "    # Ensure that indices start before the length of data - block_size to prevent out of bounds error\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i + block_size] for i in idx])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "# Grab a sample batch\n",
    "x, y = get_random_batch(train_data,)\n",
    "print('features:')\n",
    "print(x)\n",
    "print('labels')\n",
    "print(y)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c49a3bf7-4661-4471-89f2-7412a9686ac5",
   "metadata": {
    "id": "c49a3bf7-4661-4471-89f2-7412a9686ac5"
   },
   "outputs": [],
   "source": [
    "def get_batch_sequentially(data, batch_size=4, block_size=8):\n",
    "    size_of_data = len(data) - 1 # subtracting one for the label\n",
    "    for idx in range(0, size_of_data, block_size*batch_size):\n",
    "        x = torch.stack([data[idx + i: idx + block_size + i] for i in range(batch_size)])\n",
    "        y = torch.stack([data[idx + i + 1: idx + block_size + i + 1] for i in range(batch_size)])\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2949a740-5808-42bd-910d-8fb9fcf4b576",
   "metadata": {
    "id": "2949a740-5808-42bd-910d-8fb9fcf4b576"
   },
   "outputs": [],
   "source": [
    "# rnn = RNN(vocab_size, emb_dim, n_hidden)\n",
    "@torch.no_grad()\n",
    "def evaluate(model, batch_size=4, block_size=8):\n",
    "    train_loader = get_batch_sequentially(train_data, batch_size=batch_size, block_size=block_size)\n",
    "    test_loader = get_batch_sequentially(test_data,  batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "    loss_i = []\n",
    "    for loader in (train_loader, test_loader):\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                loss, _ = rnn(x, y)\n",
    "                loss_i.append(loss)\n",
    "            if loader == train_loader:\n",
    "                print(f'average training loss: {sum(loss_i)/len(loss_i):.4f}', end=' | ')\n",
    "            else:\n",
    "                print(f'average test loss: {sum(loss_i)/len(loss_i):.4f}')\n",
    "# evaluate(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb5493c1-585c-47a0-99d7-1572d60c9179",
   "metadata": {
    "id": "bb5493c1-585c-47a0-99d7-1572d60c9179"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, block_size=8, max_output_size=50):\n",
    "    # Handle prompts shorter than block_size\n",
    "    # if len(prompt) < block_size:\n",
    "    #     count_missing = block_size - len(prompt)\n",
    "    #     prompt = (count_missing * ' ') + prompt\n",
    "    prompt = 's'\n",
    "    # print(prompt, end='')\n",
    "    # trim text longer than block size\n",
    "    for i in range(max_output_size):\n",
    "        prompt_tok = encode(prompt)\n",
    "        prompt_trimmed = prompt_tok[-block_size:]\n",
    "        # print(f'input seq to model: {\"\".join(decode([i for i in prompt_trimmed]))}')\n",
    "        prompt_trimmed_tnsr = torch.tensor(prompt_tok).unsqueeze(0).to(device) # Add batch dimension\n",
    "        loss, logits = model(prompt_trimmed_tnsr)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        probs = F.softmax(logits[:, -1], dim=1)\n",
    "        idx = torch.multinomial(probs, 1, replacement=True)\n",
    "        prompt += decode([idx.item()])\n",
    "        # print(f'{prompt=}')\n",
    "        # prompt_trimmed.pop()\n",
    "        # prompt_trimmed.append(idx.item())\n",
    "    return prompt\n",
    "# print(generate(rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328cfba8-8695-4c54-8afb-46f9498d7466",
   "metadata": {
    "id": "328cfba8-8695-4c54-8afb-46f9498d7466",
    "outputId": "c660ec0b-da47-447f-e279-8e985d251253"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"shoud.\\n\\nTMREK\\nARTG:\\nSo you me hacet ment elof thol'd roudls heost may, theat nent mere haigcay winentless for of lisand sthy lomd;\\nTharght mais be wim. I be beard caghiendedt?\\n\\nPENRDDIIO:\\nI rlaven deas\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate(rnn, block_size=200, max_output_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "iBzCyLNAips5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBzCyLNAips5",
    "outputId": "a380a827-2332-4d4a-89c7-b428539753ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = torch.randn((2,2))\n",
    "foo = torch.stack((foo, foo))\n",
    "foo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ef30eeb-99b5-4e58-82ff-acb57259af48",
   "metadata": {
    "id": "2ef30eeb-99b5-4e58-82ff-acb57259af48"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, n_hidden, cell_type):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "        self.rnn = nn.GRU(self.emb_dim, self.n_hidden, 2, batch_first=True, dropout=0.5)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.hidden_state = torch.zeros((1, self.n_hidden), requires_grad=True)\n",
    "        self.linear = nn.Linear(self.n_hidden, self.vocab_size)\n",
    "\n",
    "    # Impleting with rnn from pytorch\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.emb(x)\n",
    "        b, t, e = x.shape\n",
    "\n",
    "        h_prev = self.hidden_state.expand((b, -1)) # shape: batch x n_hidden\n",
    "        h_prev = torch.stack((h_prev, h_prev)) # shape: num_layers x batch x n_hidden\n",
    "        output, hidden = self.rnn(x, h_prev.to(device)) # output shape: b x t x n_hidden; hidden shape: see previous line\n",
    "        logits = self.linear(output) # shape b x t x h_hidden --> b x t x vocab_size\n",
    "        loss = None\n",
    "        # import pdb; pdb.set_trace()\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f391408-73fd-4412-a9ac-75158223e9ef",
   "metadata": {
    "id": "2f391408-73fd-4412-a9ac-75158223e9ef",
    "outputId": "8ddf9285-8a65-4291-de4d-6c3c26e770cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=4.2010\n",
      "loss.item()=2.7721\n",
      "loss.item()=2.1375\n",
      "loss.item()=1.6474\n",
      "loss.item()=1.1980\n",
      "loss.item()=0.7883\n",
      "loss.item()=0.4686\n",
      "loss.item()=0.2564\n",
      "loss.item()=0.1356\n",
      "loss.item()=0.0762\n",
      "loss.item()=0.0484\n",
      "loss.item()=0.0331\n",
      "loss.item()=0.0251\n",
      "loss.item()=0.0204\n",
      "loss.item()=0.0174\n",
      "loss.item()=0.0153\n",
      "loss.item()=0.0134\n",
      "loss.item()=0.0113\n",
      "loss.item()=0.0102\n",
      "loss.item()=0.0092\n"
     ]
    }
   ],
   "source": [
    "### Overfitting single batch\n",
    "\n",
    "num_iters = 200\n",
    "vocab_size = 65\n",
    "emb_dim = 64\n",
    "n_hidden = 100\n",
    "block_size = 200\n",
    "batch_size = 4\n",
    "max_output_size = 150\n",
    "\n",
    "rnn = RNN(vocab_size, emb_dim, n_hidden, cell_type='rnn')\n",
    "optim = torch.optim.AdamW(rnn.parameters(), lr=0.005)#, betas=(0.9, 0.99), eps=1e-8)\n",
    "\n",
    "x, y = get_random_batch(train_data, batch_size=batch_size, block_size=block_size)\n",
    "for i in range(num_iters):\n",
    "    # x, y = get_batch(train_data, batch_size=batch_size, block_size=block_size)\n",
    "    loss, _ = rnn(x, y)\n",
    "    loss.backward()\n",
    "    if not (i % 10):\n",
    "        print(f'{loss.item()=:.4f}')\n",
    "\n",
    "    optim.step()\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29564bd5-8b28-476f-8dec-745b51495416",
   "metadata": {
    "id": "29564bd5-8b28-476f-8dec-745b51495416"
   },
   "source": [
    "Overfitting single batch with rnn implementation from scratch gets loss of 0.0027 in 200 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16cb01f-4bb9-4dad-8da5-9be17f829056",
   "metadata": {
    "id": "e16cb01f-4bb9-4dad-8da5-9be17f829056",
    "outputId": "ac56815b-8c47-4f49-8160-71714029f0bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "step: 0 | average training loss: 4.1930 | average test loss: 4.1932\n",
      "Generating a sample: \n",
      "sJgGNSW'Q!CmQx!MEcPl$.HjGA-G?CpK &tM.X$$oCaecJz-kmGKyEtkYAnx.p.EtXV.Ym$fE-zmksK;iuHECiZRaBXOSjPYj\n",
      "\n",
      "zL\n",
      "\n",
      "==============================\n",
      "--------------------------------\n",
      "step: 10 | average training loss: 2.9437 | average test loss: 2.9437\n",
      "Generating a sample: \n",
      "se te \n",
      "nm tohou\n",
      "hiiav\n",
      "wfrih osemn oml'C$pr ite en\n",
      "Io.o\n",
      "hta edne\n",
      "\n",
      "athistp \n",
      "nZofchcahn tb rdtobseoO\n",
      "Iad\n",
      "\n",
      "==============================\n",
      "--------------------------------\n",
      "step: 20 | average training loss: 2.6479 | average test loss: 2.6471\n",
      "Generating a sample: \n",
      "s wh?n to honsd:\n",
      "O\n",
      "epead k and cor motefsthafedre uyou heensokd bo fge a shl\n",
      "I-;ecanorelcocoCre iangd\n",
      "\n",
      "==============================\n",
      "--------------------------------\n",
      "step: 30 | average training loss: 2.4984 | average test loss: 2.4980\n",
      "Generating a sample: \n",
      "s?, cedt houut kiwld urd rete for herut p nhe ed yeear,\n",
      "lon\n",
      "lored\n",
      "Wane welin; as sovelAn\n",
      "Woingith,\n",
      "I \n",
      "\n",
      "==============================\n",
      "--------------------------------\n",
      "step: 40 | average training loss: 2.4062 | average test loss: 2.4054\n",
      "Generating a sample: \n",
      "s.\n",
      "\n",
      "IRNQDKAKPSV:\n",
      "Whit Iel werel, \n",
      "RIRC:\n",
      "uRt al Eans I: mn suchmear hathe be.\n",
      "\n",
      "\n",
      "OM;UEMI EFwyNCUEEETUSo\n",
      "\n",
      "==============================\n",
      "--------------------------------\n",
      "step: 50 | average training loss: 2.3292 | average test loss: 2.3288\n",
      "Generating a sample: \n",
      "spatool tharay\n",
      "That lhe wanecinm nadwed ledo fowe,, foontadsbulithe winlinmens to bisir rielb wale th\n",
      "\n",
      "==============================\n",
      "--------------------------------\n",
      "step: 60 | average training loss: 2.2808 | average test loss: 2.2810\n",
      "Generating a sample: \n",
      "s's of brud and I.\n",
      "ADNpd roon gaigs no risp thuill los I cund gay gore\n",
      "Freldg thy feave cohad? as dio\n",
      "\n",
      "==============================\n",
      "--------------------------------\n",
      "step: 70 | average training loss: 2.2354 | average test loss: 2.2357\n",
      "Generating a sample: \n",
      "sfuidt purthut my rell\n",
      "Iiowt I is miet\n",
      "And sis 'd he nandedstert,\n",
      "Anccore you he wisn.\n",
      "\n",
      "EORDMI:\n",
      "NoSA:\n",
      "\n",
      "==============================\n",
      "--------------------------------\n",
      "step: 80 | average training loss: 2.1956 | average test loss: 2.1965\n",
      "Generating a sample: \n",
      "sriess 'ingn, ettin vigh\n",
      "Tage noy hiss sorusefer thei'er, ther hat trark, roarth. vorety berverevse t\n",
      "\n",
      "==============================\n",
      "--------------------------------\n",
      "step: 90 | average training loss: 2.1678 | average test loss: 2.1694\n",
      "Generating a sample: \n",
      "sculings mith fil yitht bait whamgdy the ore pray, crin me gipatt in and ousTer eam, raes benese whar\n",
      "\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_iters = 100\n",
    "vocab_size = 65\n",
    "emb_dim = 64\n",
    "n_hidden = 100\n",
    "block_size = 200\n",
    "batch_size = 4\n",
    "max_output_size = 150\n",
    "\n",
    "# Initialize the model\n",
    "rnn = RNN(vocab_size, emb_dim, n_hidden, batch_size)\n",
    "\n",
    "# Training at a higher learning rate\n",
    "optim = torch.optim.AdamW(rnn.parameters(), lr=0.005)\n",
    "\n",
    "# Storing loss\n",
    "lossi = []\n",
    "stepi = []\n",
    "\n",
    "for i in range(num_iters):\n",
    "    x, y = get_random_batch(train_data, batch_size=batch_size, block_size=block_size)\n",
    "    loss, _ = rnn(x, y)\n",
    "    loss.backward()\n",
    "\n",
    "    if not (i % 10):\n",
    "        print('--------------------------------')\n",
    "        print(f'step: {i}', end=' | ')\n",
    "        evaluate(rnn, batch_size=batch_size, block_size=block_size)\n",
    "        print(\"Generating a sample: \")\n",
    "        print(generate(rnn, block_size=block_size, max_output_size=100))\n",
    "        print(\"\\n==============================\")\n",
    "    lossi.append(loss)\n",
    "    stepi.append(i)\n",
    "\n",
    "    optim.step()\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ac96768",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ac96768",
    "outputId": "4e4a3124-d9bf-418c-9164-e4620b1bd555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 | average training loss: 4.1671 | average test loss: 4.1667\n",
      "iter: 0: train loss: 4.1578\n",
      "Generating a sample: \n",
      "sRXHkjp?bcEcaBJE&pzpPSN,MtEth$aWbC-; xkJuGHDT!fCgr'cp DJZb-rftYl;NwZIoayL;V\n",
      "y'UMqUtJ3 bal$zlzoo:p?B,MM:LAyvm.Ux QShO&Xfpl.IjUxBAip-HfWRoKlFTyJTu!vbG3ha\n",
      "step: 1000 | average training loss: 1.9404 | average test loss: 1.9491\n",
      "iter: 1000: train loss: 1.6392\n",
      "Generating a sample: \n",
      "swert it, where to prock', in himg?\n",
      "\n",
      "ONDY: Ond to is and out bid?\n",
      "DUKY:\n",
      "Hard than he eed revourrest porth your this laing in his sporth of his wood:\n",
      "Hu\n",
      "step: 2000 | average training loss: 1.8593 | average test loss: 1.8703\n",
      "iter: 2000: train loss: 1.8941\n",
      "Generating a sample: \n",
      "slof yet.\n",
      "\n",
      "PRICHARD II:\n",
      "I may him are gis a why, him like that to the wick nput for as und him give of remosclems I'll for revences thow how dost that \n",
      "step: 3000 | average training loss: 1.8281 | average test loss: 1.8419\n",
      "iter: 3000: train loss: 1.9971\n",
      "Generating a sample: \n",
      "sais\n",
      "Oft trough by himsels.\n",
      "\n",
      "Sell.\n",
      "Comourns their cordrey us Weak's kever to he man' look;\n",
      "What hace as I your we sublefeciress orn in to I persen, by \n",
      "step: 4000 | average training loss: 1.7905 | average test loss: 1.8055\n",
      "iter: 4000: train loss: 1.7168\n",
      "Generating a sample: \n",
      "s frrim nile could by Marce,\n",
      "To ray agoolad, this feer to the uke out ntal' feetains and Romeour the ore my dood us no like ome\n",
      "That forget be?!\n",
      "\n",
      "HANIO\n",
      "step: 5000 | average training loss: 1.7879 | average test loss: 1.8040\n",
      "iter: 5000: train loss: 1.7864\n",
      "Generating a sample: \n",
      "strey wenwal.\n",
      "Whose bloters bounds thy micustiugntilys of me must know'\n",
      "Frmentted his do not blows Isay is nothoushing?\n",
      "No poorroused thy see, mastregr\n",
      "step: 6000 | average training loss: 1.7757 | average test loss: 1.7913\n",
      "iter: 6000: train loss: 1.8676\n",
      "Generating a sample: \n",
      "shamply lan\n",
      "A male in trust not upon list?\n",
      "\n",
      "Secanlialt.\n",
      "\n",
      "Secon Cold:\n",
      "But, yet any drow constry advener all afin'd sword sles over could thy like,\n",
      "The o\n",
      "step: 7000 | average training loss: 1.7698 | average test loss: 1.7846\n",
      "iter: 7000: train loss: 1.7012\n",
      "Generating a sample: \n",
      "shand the kinl-then mays,\n",
      "Thy chaerter falall wich you, which know?\n",
      "\n",
      "GLOUCESTER:\n",
      "Let have but lant if so malfoud ther chaman. We are the day ups' the m\n",
      "step: 8000 | average training loss: 1.7459 | average test loss: 1.7612\n",
      "iter: 8000: train loss: 1.6184\n",
      "Generating a sample: \n",
      "s\n",
      "Rexluc, thee.\n",
      "\n",
      "ThES:\n",
      "Swecks it, me that Jure'ful DUIS:\n",
      "Co her in these precoins nothof in and what might-alwick of allackwait\n",
      "\n",
      "Hold: of me mout lief \n",
      "step: 9000 | average training loss: 1.7534 | average test loss: 1.7674\n",
      "iter: 9000: train loss: 1.9551\n",
      "Generating a sample: \n",
      "s form.\n",
      "\n",
      "LADY Apost I if thee; thy sorran\n",
      "To semal it.\n",
      "\n",
      "HASTINGS:\n",
      "A trenget procial in ord Yowes die; efcy heab-life then of my wally not the weep,\n",
      "He\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_iters = 10000\n",
    "vocab_size = 65\n",
    "emb_dim = 64\n",
    "n_hidden = 100\n",
    "block_size = 32\n",
    "batch_size = 4\n",
    "max_output_size = 150\n",
    "\n",
    "# Initialize the model\n",
    "rnn = RNN(vocab_size, emb_dim, n_hidden, batch_size).to(device)\n",
    "# optim = torch.optim.AdamW(rnn.parameters(), .001)\n",
    "# Training at a higher learning rate\n",
    "optim = torch.optim.AdamW(rnn.parameters(), lr=0.005)\n",
    "\n",
    "# Storing loss\n",
    "lossi = []\n",
    "stepi = []\n",
    "# Storing hidden_state\n",
    "hidden_states = []\n",
    "\n",
    "for i in range(num_iters):\n",
    "    x, y = get_random_batch(train_data, batch_size=batch_size, block_size=block_size)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    loss, _ = rnn(x, y)\n",
    "    loss.backward()\n",
    "\n",
    "    if not (i % 1000):\n",
    "        print(f'step: {i}', end=' | ')\n",
    "        evaluate(rnn, batch_size=batch_size, block_size=block_size)\n",
    "        print(f'iter: {i}: train loss: {loss:.4f}')\n",
    "        print('Generating a sample: ')\n",
    "        print(generate(rnn, block_size=block_size, max_output_size=max_output_size))\n",
    "    lossi.append(loss)\n",
    "    stepi.append(i)\n",
    "    hidden_states.append(rnn.hidden_state)\n",
    "\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "LOmt4_AcewXV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOmt4_AcewXV",
    "outputId": "850859e0-48ef-4a2a-b738-31ce79441760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 | average training loss: 4.1745 | average test loss: 4.1745\n",
      "Generating a sample: \n",
      "sAlQDK'qi?XWRc:cGydQ:VCzBDS3rpVvWRXKHExaN!JSCGtodG  H-'UcArzaZgnV;np;-ObKm!-Y-zv'WLGZsfwg vUoTjlLSKK'qkmZddgx3NRv!Lhkylzk.!duR& ,I3qfyzRzH?JTF\n",
      "ijG?ojA,\n",
      "\n",
      "===============================================\n",
      "step: 10000 | average training loss: 1.4540 | average test loss: 1.4756\n",
      "Generating a sample: \n",
      "s.\n",
      "Come, we to well: much of an ingignation!\n",
      "Breat worthy gradies! muse or taken elding:\n",
      "That house, who it be wild holy nor day\n",
      "His king, being it die\n",
      "\n",
      "===============================================\n",
      "step: 20000 | average training loss: 1.4308 | average test loss: 1.4519\n",
      "Generating a sample: \n",
      "s dishen ales to than?\n",
      "\n",
      "Volzarcemity,\n",
      "Eargies; than our gnave God's not would do\n",
      "Destruy, do put our Marcius: than mean that tragispiedy baning he thy \n",
      "\n",
      "===============================================\n",
      "step: 30000 | average training loss: 1.4245 | average test loss: 1.4458\n",
      "Generating a sample: \n",
      "s unput thee, she will so yet.\n",
      "\n",
      "Clown:\n",
      "Yet my frimits ever dear himself, it is,\n",
      "Who is mispoints his merely short,\n",
      "His love, meath, that thy hearing ap\n",
      "\n",
      "===============================================\n",
      "step: 40000 | average training loss: 1.4221 | average test loss: 1.4448\n",
      "Generating a sample: \n",
      "s!?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Men'd and bloody a crate of thee ride!\n",
      "\n",
      "Provost:\n",
      "His tene dear unstickling;\n",
      "All have regrestladsorta,\n",
      "And hope; Crocold mortal he is s\n",
      "\n",
      "===============================================\n",
      "step: 50000 | average training loss: 1.4151 | average test loss: 1.4361\n",
      "Generating a sample: \n",
      "s betweel not;\n",
      "With play one dievily,\n",
      "You think a generaltee's and in the grace,\n",
      "As I wouchsed than his friends\n",
      "in this sign of Epens it that weigh;\n",
      "Th\n",
      "\n",
      "===============================================\n",
      "step: 60000 | average training loss: 1.4117 | average test loss: 1.4336\n",
      "Generating a sample: \n",
      "sment is his sould time?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "You responer up ye shall remember, to what's\n",
      "Which doop! 'Twhich hanging poor sorrow are I do their grieg\n",
      "No\n",
      "\n",
      "===============================================\n",
      "step: 70000 | average training loss: 1.4090 | average test loss: 1.4303\n",
      "Generating a sample: \n",
      "severy for labour'd off them.\n",
      "\n",
      "SICINIUS:\n",
      "Hadly redleon, sir, proper and have\n",
      "Who, Lanticiind that, heavy more their honour liberty.\n",
      "\n",
      "PAULINA:\n",
      "Net than \n",
      "\n",
      "===============================================\n",
      "step: 80000 | average training loss: 1.4079 | average test loss: 1.4305\n",
      "Generating a sample: \n",
      "sfecty pardined with oad,\n",
      "Your love, barment and lighting good fruck!\n",
      "Our where, did not pazy. Fares me, and he love?\n",
      "\n",
      "Both Romeo\n",
      "You be stern off your\n",
      "\n",
      "===============================================\n",
      "step: 90000 | average training loss: 1.4089 | average test loss: 1.4299\n",
      "Generating a sample: \n",
      "s account and side him:\n",
      "I will answer what lend, one pray thes precience, another your pound,\n",
      "For yock and nor I may heaven,\n",
      "And a tent.\n",
      "\n",
      "LEONTES:\n",
      "You,\n",
      "\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_iters = 100_000\n",
    "vocab_size = 65\n",
    "emb_dim = 64\n",
    "n_hidden = 100\n",
    "block_size = 200\n",
    "batch_size = 8\n",
    "max_output_size = 150\n",
    "\n",
    "# device (cpu or gpu)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the model\n",
    "rnn = RNN(vocab_size, emb_dim, n_hidden, batch_size).to(device)\n",
    "# optim = torch.optim.AdamW(rnn.parameters(), .001)\n",
    "# Training at a higher learning rate\n",
    "optim = torch.optim.AdamW(rnn.parameters(), lr=0.005)\n",
    "\n",
    "# Storing loss\n",
    "lossi = []\n",
    "stepi = []\n",
    "# Storing hidden_state\n",
    "hidden_states = []\n",
    "\n",
    "for i in range(num_iters):\n",
    "    x, y = get_random_batch(train_data, batch_size=batch_size, block_size=block_size)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    loss, _ = rnn(x, y)\n",
    "    loss.backward()\n",
    "\n",
    "    if not (i % 20000):\n",
    "        print(f'step: {i}', end=' | ')\n",
    "        evaluate(rnn, batch_size=batch_size, block_size=block_size)\n",
    "        # print(f'iter: {i}: train loss: {loss:.4f}')\n",
    "        print('Generating a sample: ')\n",
    "        print(generate(rnn, block_size=block_size, max_output_size=max_output_size))\n",
    "        print('\\n===============================================')\n",
    "    lossi.append(loss)\n",
    "    stepi.append(i)\n",
    "    hidden_states.append(rnn.hidden_state)\n",
    "\n",
    "    optim.step()\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d70956fb-0002-42b9-90db-23093d2b8ba0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d70956fb-0002-42b9-90db-23093d2b8ba0",
    "outputId": "1d738516-4514-471b-b445-055ce5d5c902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 | average training loss: 4.2053 | average test loss: 4.2052\n",
      "Generating a sample: \n",
      "sTmxroxubvUU,LaduF,P !&-:&;!mjsPD$,lzSDNtV VZRm$QX3w?N!zBToRzJgpz \n",
      "!.ThF!S?uH,t!QU\n",
      "dMfpPsy\n",
      "gA:v3 obyRFETByVKwkPVQYpGWukdbxP hhvZNRE&jC3A?jYBfRzwm3uF;u'\n",
      "\n",
      "===============================================\n",
      "step: 20000 | average training loss: 1.6093 | average test loss: 1.6280\n",
      "Generating a sample: \n",
      "s,\n",
      "Mespic nighan. Can bear gentle look'd me:\n",
      "Trukeme oft yow thint! this heards the king.\n",
      "\n",
      "Sicurerain:\n",
      "To you'll look-seasim? I\n",
      "that yet thus: if it tr\n",
      "\n",
      "===============================================\n",
      "step: 40000 | average training loss: 1.5836 | average test loss: 1.6017\n",
      "Generating a sample: \n",
      "ssement of lay.\n",
      "You, lieking frant-maven one have but bend them,\n",
      "bucked to be it me,\n",
      "I'ly he's stelp I thee had for fiocrong aste hisgety.\n",
      "\n",
      "CLARENCE:\n",
      "D\n",
      "\n",
      "===============================================\n",
      "step: 60000 | average training loss: 1.5676 | average test loss: 1.5866\n",
      "Generating a sample: \n",
      "se from my braashed, and I king,\n",
      "Comeing my farth with a comprabe, becomed's our blood:\n",
      "His upon substaal strige, and King, 'Twixt both this?\n",
      "Be kend t\n",
      "\n",
      "===============================================\n",
      "step: 80000 | average training loss: 1.5551 | average test loss: 1.5749\n",
      "Generating a sample: \n",
      "sh this pride,\n",
      "To mine open stald parrot it, below'd and fall\n",
      "A gann now city she dgupwert hath dishied of the\n",
      "breath, die not not rughing\n",
      "do cityweld \n",
      "\n",
      "===============================================\n",
      "step: 100000 | average training loss: 1.5458 | average test loss: 1.5646\n",
      "Generating a sample: \n",
      "sorrous on and 'scaditoadal,\n",
      "And that most I was night as ats Engur thee,\n",
      "Planden'd tresterthing be\n",
      "To his eurcem bushe his and late;\n",
      "One you; so thou \n",
      "\n",
      "===============================================\n",
      "step: 120000 | average training loss: 1.5399 | average test loss: 1.5596\n",
      "Generating a sample: \n",
      "shat,\n",
      "Which that I be na\n",
      "bove of the eathed should stats\n",
      "With order.\n",
      "\n",
      "CLowE:\n",
      "You ragenioul to affes; were god brosor and his mother in you list\n",
      "Englist\n",
      "\n",
      "===============================================\n",
      "step: 140000 | average training loss: 1.5300 | average test loss: 1.5492\n",
      "Generating a sample: \n",
      "saled,--\n",
      "\n",
      "ESCALUS:\n",
      "Wherewomed thee?\n",
      "\n",
      "PLINCESTEN:\n",
      "We praymy flioh, in those in Nass.\n",
      "\n",
      "Second Werflvand:\n",
      "O what was no celit.\n",
      "\n",
      "Secondy Lord:\n",
      "But out this\n",
      "\n",
      "===============================================\n",
      "step: 160000 | average training loss: 1.5270 | average test loss: 1.5462\n",
      "Generating a sample: \n",
      "sore,\n",
      "And be depers'd thee sweat him nender truas.\n",
      "\n",
      "MENENIUS:\n",
      "Go made;\n",
      "Give new on a reitsels madkin make-should hear lutcher:\n",
      "Let honour feeling? I wo\n",
      "\n",
      "===============================================\n",
      "step: 180000 | average training loss: 1.5224 | average test loss: 1.5418\n",
      "Generating a sample: \n",
      "s-ofiented ere bromelthihion:\n",
      "The truth of with Are woudted with them in the noble;\n",
      "Andries with friar!\n",
      "\n",
      "PAULINE:\n",
      "What brothers pray nameing a very ter\n",
      "\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_iters = 200_000\n",
    "vocab_size = 65\n",
    "emb_dim = 64\n",
    "n_hidden = 100\n",
    "block_size = 200\n",
    "batch_size = 8\n",
    "max_output_size = 150\n",
    "\n",
    "# device (cpu or gpu)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the model\n",
    "rnn = RNN(vocab_size, emb_dim, n_hidden, batch_size).to(device)\n",
    "# optim = torch.optim.AdamW(rnn.parameters(), .001)\n",
    "# Training at a higher learning rate\n",
    "optim = torch.optim.AdamW(rnn.parameters(), lr=0.005)\n",
    "\n",
    "# Storing loss\n",
    "lossi = []\n",
    "stepi = []\n",
    "# Storing hidden_state\n",
    "hidden_states = []\n",
    "\n",
    "for i in range(num_iters):\n",
    "    x, y = get_random_batch(train_data, batch_size=batch_size, block_size=block_size)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    loss, _ = rnn(x, y)\n",
    "    loss.backward()\n",
    "\n",
    "    if not (i % 20000):\n",
    "        print(f'step: {i}', end=' | ')\n",
    "        evaluate(rnn, batch_size=batch_size, block_size=block_size)\n",
    "        # print(f'iter: {i}: train loss: {loss:.4f}')\n",
    "        print('Generating a sample: ')\n",
    "        print(generate(rnn, block_size=block_size, max_output_size=max_output_size))\n",
    "        print('\\n===============================================')\n",
    "    lossi.append(loss)\n",
    "    stepi.append(i)\n",
    "    hidden_states.append(rnn.hidden_state)\n",
    "\n",
    "    optim.step()\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "L5DTIjOykCTN",
   "metadata": {
    "id": "L5DTIjOykCTN"
   },
   "outputs": [],
   "source": [
    "# torch.save(rnn, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "JtnXrEmKmv4v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtnXrEmKmv4v",
    "outputId": "13cee16c-2013-48ee-acfa-90f7eb883b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sew must a Banionable.\n",
      "\n",
      "CARULIO:\n",
      "Tent, beheed thin water, on Wirie'd?\n",
      "\n",
      "LARTIUS:\n",
      "Caminmed, thou may away, to-more on haste,\n",
      "Ind ored oullronsh'd beal those sir,\n",
      "Lord I cannot's, for i' out-to should bies.\n",
      "\n",
      "QUEEN ELIZARET:\n",
      "Sir, renpest that him 'Go will right bit you recrane,\n",
      "Andee--Goors disbe lewe as arring.\n",
      "I sweetn he of her: as my off quemen that\n",
      "Singer it as vwinous house,\n",
      "No eagles,\n",
      "and for the cortioninkly, that a\n",
      "correased she for you have all the fnesord a bate\n",
      "With speak al one, my trent\n"
     ]
    }
   ],
   "source": [
    "print(generate(rnn, max_output_size=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3201e7c-6908-4525-882b-57d91261664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "# loaded_model = torch.load('model.pth', weights_only=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
