{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8eaf532-f95b-475d-9a3c-a43cdbf0bb9d",
   "metadata": {},
   "source": [
    "### Build and train a simple Recurrent Neural Network\n",
    "We are going to use writings of shakespear as our data.\n",
    "\n",
    "Steps:\n",
    "- Download the data\n",
    "- Preprocessing the data\n",
    "- Create character tokenizer\n",
    "- Tokenize data\n",
    "- Split data (train / test)\n",
    "- Create a batches of data to be fed to the model\n",
    "- Define parameters for the model\n",
    "- Initialize the model\n",
    "- Overfit a single batch\n",
    "- Train the model\n",
    "- Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "995f77ce-1d55-41e7-a672-03aa81e97231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d435ded-dedc-4589-8897-5b7e241f433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data \n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2f339b-37c2-4667-9ed1-6af1f38ac1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7297bac4-93be-4907-810e-f5c1f7169e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of characters: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26294ebc-d7a9-45f7-b119-de4d918040b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'Number of unique characters: {vocab_size}')\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a276a0f-6696-4e69-b160-8439ea79187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43, 2]\n",
      "hello there!\n"
     ]
    }
   ],
   "source": [
    "# Character level tokenizer\n",
    "stoi = {char: idx for idx, char in enumerate(chars)}\n",
    "itos = {idx: char for char, idx in stoi.items()}\n",
    "\n",
    "encode = lambda s: [stoi[char] for char in s]\n",
    "decode = lambda i: ''.join([itos[idx] for idx in i])\n",
    "\n",
    "print(encode('hello there!'))\n",
    "print(decode(encode('hello there!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65088e39-5829-4e24-b94d-0683fd62086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all text\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb208371-fc18-4e14-8001-332ba7e2045b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looking at some raw text \n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "005e0dc7-8934-49e9-a73a-c1052624b4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at tokenized text\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d43f06-4825-44ac-beaf-1b1839dce0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, test_pct=.1):\n",
    "    data_size = len(data)\n",
    "    num_test_samples = int(data_size * test_pct)\n",
    "    test_data = data[-num_test_samples:]\n",
    "    train_data = data[:data_size - num_test_samples]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61ec0be6-9bf7-4df4-8625-36470cf19c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set: 1003855\n",
      "size of test set: 111539\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data)\n",
    "print(f'size of training set: {len(train_data)}')\n",
    "print(f'size of test set: {len(test_data)}')\n",
    "\n",
    "assert len(train_data) + len(test_data) == len(data), \"Lost data during train test split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69841c3d-c8f4-42a1-9573-cfe6230729ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:\n",
      "tensor([[57, 47, 52, 45,  1, 41, 53, 61],\n",
      "        [ 1, 57, 59, 41, 46,  1, 39,  1],\n",
      "        [37,  1, 34, 21,  0,  0, 23, 21],\n",
      "        [43,  1, 40, 39, 40, 43,  6,  0]])\n",
      "labels\n",
      "tensor([[47, 52, 45,  1, 41, 53, 61, 39],\n",
      "        [57, 59, 41, 46,  1, 39,  1, 41],\n",
      "        [ 1, 34, 21,  0,  0, 23, 21, 26],\n",
      "        [ 1, 40, 39, 40, 43,  6,  0, 21]])\n",
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "# Function to get a batch\n",
    "def get_random_batch(data, batch_size=4, block_size=8):\n",
    "    # Get  batch_size of random indices which will be the start of sequence\n",
    "    # Ensure that indices start before the length of data - block_size to prevent out of bounds error\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i + block_size] for i in idx])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "# Grab a sample batch\n",
    "x, y = get_random_batch(train_data,)\n",
    "print('features:')\n",
    "print(x)\n",
    "print('labels')\n",
    "print(y)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c49a3bf7-4661-4471-89f2-7412a9686ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_sequentially(data, batch_size=4, block_size=8):\n",
    "    size_of_data = len(data) - 1 # subtracting one for the label \n",
    "    for idx in range(0, size_of_data, block_size*batch_size):\n",
    "        x = torch.stack([data[idx + i: idx + block_size + i] for i in range(batch_size)])\n",
    "        y = torch.stack([data[idx + i + 1: idx + block_size + i + 1] for i in range(batch_size)])\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2949a740-5808-42bd-910d-8fb9fcf4b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn = RNN(vocab_size, emb_dim, n_hidden)\n",
    "@torch.no_grad()\n",
    "def evaluate(model, batch_size=4, block_size=8):\n",
    "    train_loader = get_batch_sequentially(train_data, batch_size=batch_size, block_size=block_size)\n",
    "    test_loader = get_batch_sequentially(test_data,  batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "    loss_i = []\n",
    "    for loader in (train_loader, test_loader):\n",
    "            for x, y in loader:\n",
    "                loss, _ = rnn(x, y)\n",
    "                loss_i.append(loss)\n",
    "            if loader == train_loader:    \n",
    "                print(f'average training loss: {sum(loss_i)/len(loss_i):.4f}', end=' | ')\n",
    "            else:\n",
    "                print(f'average test loss: {sum(loss_i)/len(loss_i):.4f}')\n",
    "# evaluate(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb5493c1-585c-47a0-99d7-1572d60c9179",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(prompt, model, block_size=8, max_output_size=50):\n",
    "    # Handle prompts shorter than block_size\n",
    "    if len(prompt) < block_size:\n",
    "        count_missing = block_size - len(prompt)\n",
    "        prompt = (count_missing * ' ') + prompt\n",
    "    prompt_tok = encode(prompt)\n",
    "    print(prompt, end='')\n",
    "    # trim text longer than block size\n",
    "    prompt_trimmed = prompt_tok[-block_size:]\n",
    "    for i in range(max_output_size):\n",
    "        prompt_trimmed_tnsr = torch.tensor(prompt_trimmed).unsqueeze(0)\n",
    "        loss, logits = rnn(prompt_trimmed_tnsr)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        probs = F.softmax(logits[:, block_size-1], dim=1)\n",
    "        idx = torch.multinomial(probs, 1, replacement=True)\n",
    "        print(decode([idx.item()]), end='')\n",
    "        prompt_trimmed.pop()\n",
    "        prompt_trimmed.append(idx.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ef30eeb-99b5-4e58-82ff-acb57259af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, emb_dim, n_hidden):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(emb_dim+n_hidden, n_hidden)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # print(f'{x.shape=}, {hidden.shape=}, {self.linear.weight.shape=}')\n",
    "        h = torch.cat((x, hidden), dim=1) # shape b x emb_dim + n_hidden\n",
    "        # print(f'{h.shape=}')\n",
    "        return self.linear(h) # shape b x n_hidden\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, n_hidden):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "        self.rnn = RNNCell(self.emb_dim, self.n_hidden)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.hidden_state = torch.zeros((1, self.n_hidden), requires_grad=True)\n",
    "        self.linear = nn.Linear(self.n_hidden, self.vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.emb(x)\n",
    "        b, t, e = x.shape # shape: batch x block_size x emb_dim\n",
    "\n",
    "        h = self.hidden_state.expand((b, -1)) # shape: batch x n_hidden\n",
    "        list_hidden_states = []\n",
    "        # Pass tokens through the model\n",
    "        for i in range(t):\n",
    "            # h = h + self.tanh(self.rnn(x[:, i, :])) # dim after slicing: b x emb --> output shape: b x n_hidden\n",
    "            x_t = x[:, i, :]\n",
    "            h = self.tanh(self.rnn(x_t, h)) # h's shape: b x n_hidden\n",
    "            list_hidden_states.append(h)\n",
    "\n",
    "        # hidden acts is a list of (b x n_hidden) (t items in the list)\n",
    "        # hidden shape = 4 x 100 | list_hiddden_states length = 8\n",
    "        # pull all cols (each row starting from 0)\n",
    "        # 4 x 8 x 100\n",
    "        # stacking it on dim=1 results in: b x t x n_hidden\n",
    "        hidden_acts = torch.stack(list_hidden_states, dim=1) # .view(b, t, self.n_hidden) # shape token, b x n_hidden -> b x t x n_hidden\n",
    "        # if stacked with dim=0 --> t x b x n_hidden\n",
    "        logits = self.linear(hidden_acts) # shape b x t x h_hidden --> b x t x vocab_size\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f391408-73fd-4412-a9ac-75158223e9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(4.1410, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(0.0786, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(0.0042, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(0.0012, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(0.0007, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(0.0005, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(0.0004, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(0.0004, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(0.0003, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(0.0003, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### Overfitting single batch\n",
    "\n",
    "num_iters = 100\n",
    "vocab_size = 65\n",
    "emb_dim = 64\n",
    "n_hidden = 100\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_output_size = 150\n",
    "\n",
    "rnn = RNN(vocab_size, emb_dim, n_hidden)\n",
    "# optim = torch.optim.AdamW(rnn.parameters(), .001)\n",
    "optim = torch.optim.AdamW(rnn.parameters(), lr=0.01, betas=(0.9, 0.99), eps=1e-8)\n",
    "\n",
    "x, y = get_random_batch(train_data, batch_size=batch_size, block_size=block_size)\n",
    "for i in range(num_iters):\n",
    "    # x, y = get_batch(train_data, batch_size=batch_size, block_size=block_size)\n",
    "    loss, _ = rnn(x, y)\n",
    "    loss.backward()\n",
    "    if not (i % 10):\n",
    "        print(f'{loss=}')\n",
    "\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    # if not (i % 10000):\n",
    "    #     print(f'step: {i}', end=' | ')\n",
    "    #     evaluate(rnn, batch_size=batch_size, block_size=block_size)\n",
    "    #     # print(f'iter: {i}: train loss: {loss:.4f}')\n",
    "    #     print('Generating a sample: ')\n",
    "    #     generate(\"Verily my lord \", rnn, block_size=block_size, max_output_size=max_output_size)\n",
    "    #     print('\\n====================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4356e62f-6af1-470a-8475-0e978e2773fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_iters = 20000\n",
    "vocab_size = 65\n",
    "emb_dim = 64\n",
    "n_hidden = 100\n",
    "block_size = 32\n",
    "batch_size=8\n",
    "max_output_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78b60d97-4db2-4de8-8c4c-dcae4f457833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 | average training loss: 4.1713 | average test loss: 4.1714\n",
      "step: 1000 | average training loss: 1.9898 | average test loss: 2.0017\n",
      "step: 2000 | average training loss: 1.9496 | average test loss: 1.9622\n",
      "step: 3000 | average training loss: 1.9518 | average test loss: 1.9643\n",
      "step: 4000 | average training loss: 1.9369 | average test loss: 1.9514\n",
      "step: 5000 | average training loss: 1.9096 | average test loss: 1.9232\n",
      "step: 6000 | average training loss: 1.9237 | average test loss: 1.9386\n",
      "step: 7000 | average training loss: 1.9172 | average test loss: 1.9319\n",
      "step: 8000 | average training loss: 1.9129 | average test loss: 1.9265\n",
      "step: 9000 | average training loss: 1.9066 | average test loss: 1.9224\n",
      "step: 10000 | average training loss: 1.9020 | average test loss: 1.9166\n",
      "step: 11000 | average training loss: 1.9251 | average test loss: 1.9420\n",
      "step: 12000 | average training loss: 1.9226 | average test loss: 1.9379\n",
      "step: 13000 | average training loss: 1.9334 | average test loss: 1.9495\n",
      "step: 14000 | average training loss: 1.9076 | average test loss: 1.9240\n",
      "step: 15000 | average training loss: 1.9055 | average test loss: 1.9191\n",
      "step: 16000 | average training loss: 1.9372 | average test loss: 1.9526\n",
      "step: 17000 | average training loss: 1.9169 | average test loss: 1.9325\n",
      "step: 18000 | average training loss: 1.9237 | average test loss: 1.9367\n",
      "step: 19000 | average training loss: 1.9014 | average test loss: 1.9156\n",
      "Generating a sample: \n",
      "                 Verily my lord traro onerak arenerinengenerines ak s a!\n",
      "My wery t:\n",
      "S inenenanenonges a:\n",
      "Tery y'derineringma, t nenonak mononaraned maineras wak.\n",
      "'s\n",
      "I\n",
      "S s, iak,\n",
      "I t h"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "rnn = RNN(vocab_size, emb_dim, n_hidden)\n",
    "# optim = torch.optim.AdamW(rnn.parameters(), .001)\n",
    "# Training at a higher learning rate\n",
    "optim = torch.optim.AdamW(rnn.parameters(), lr=0.01, betas=(0.9, 0.99), eps=1e-8)\n",
    "\n",
    "# Storing loss\n",
    "lossi = []\n",
    "stepi = []\n",
    "\n",
    "for i in range(num_iters):\n",
    "    x, y = get_random_batch(train_data, batch_size=batch_size, block_size=block_size)\n",
    "    loss, _ = rnn(x, y)\n",
    "    loss.backward()\n",
    "\n",
    "    if not (i % 1000):\n",
    "        print(f'step: {i}', end=' | ')\n",
    "        evaluate(rnn, batch_size=batch_size, block_size=block_size)\n",
    "        # print(f'iter: {i}: train loss: {loss:.4f}')\n",
    "    lossi.append(loss)\n",
    "    stepi.append(i)\n",
    "    \n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "print('Generating a sample: ')\n",
    "generate(\"Verily my lord \", rnn, block_size=block_size, max_output_size=max_output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39032589-af97-4323-8cc9-f8630cfc6e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the loss\n",
    "plt.plot(stepi, [i.item() for i in lossi]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "067c7841-3453-4f84-958c-ab12906e3d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a splendid day t ountugesour cercened:\n",
      "Ob dishes thtiry ty s hes b ce s\n",
      "he s tine tids is the; i, blfrs, atathI b, che hts s y 's minenete, the, p aiBut iuce -f rus\n",
      "Wy s.\n",
      "O ts tifes o sislde cb tusugheisis.\n",
      "D cho, ces pese cervere, t tomatidas bse bfaht y t ouorid y atzhe: s t Me.\n",
      " ce ctin\n",
      "is tas mis w t\n",
      "e mishehased chis ne blges; bals min;\n",
      "Hkserdis:\n",
      "'s htis s fed, he s ck Hesl ng nast ous much:\n",
      "Ot hesothes\n",
      "hty thtchti, chhtarhtis ot Ches lll bcero t\n",
      "\n",
      "Os h ot y, ig:\n",
      "\n",
      "O g s chestose t oue t:\n",
      "y ty: ose be cheni"
     ]
    }
   ],
   "source": [
    "generate('This is a splendid day ', rnn, max_output_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fa96d02-a6bb-4919-a5c3-51c6e3ff5de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 | average training loss: 4.1218 | average test loss: 4.1220\n",
      "Generating a sample: \n",
      "Verily my lord Lfl-qffrVbD:ZMg\n",
      "wDZ,B$ Ipy!J;wg?uFTjD,IJPfTdkUKhcyMT-ig faex:;r?qCLYOHXAdWj&XYNnMDRjrP\n",
      "jbPzoSBMsBcfTWKsrgBnY!eXV$aa&z:I,fr,SBNxWYYQseaS!nMcEhgf!sN NlPSHynmooZXiUrDw!krb.Q;diU!bhcmbfFulZZEhxYcy?KsEPyjW\n",
      "====================================\n",
      "step: 10000 | average training loss: 2.4942 | average test loss: 2.4967\n",
      "Generating a sample: \n",
      "Verily my lord mbs watant turar t my n he hted nd an n gw thakero ithit.\n",
      "Tin'd;\n",
      "An'aallath, kk t n merurat h brertlan t g\n",
      "A menem arera.\n",
      "A o,\n",
      "I\n",
      "Cat ttn\n",
      "A hiat ht p m:\n",
      "Y m by I hicen n tak\n",
      "Ag m m me n m An turenedy t\n",
      "====================================\n",
      "step: 20000 | average training loss: 2.5015 | average test loss: 2.5058\n",
      "Generating a sample: \n",
      "Verily my lord dl gllll mech d dellllanallighil mell teus thlll bll bler orgelally pl mimill gha hs d s l cal ilar y Lobely te mllell; had d my g ly hhiad ml dlgl.\n",
      "Ll l y mhillaly d olldellfr cechmy tellllllr thalav\n",
      "====================================\n",
      "step: 30000 | average training loss: 2.5356 | average test loss: 2.5386\n",
      "Generating a sample: \n",
      "Verily my lord p melis\n",
      "S, a, IIs arin,\n",
      "IIIII,\n",
      "oerearere, ve Mobelo ve e anareoccu, s:\n",
      "Y,\n",
      "Te,\n",
      "UTATos we;\n",
      "E:\n",
      "T:\n",
      "w h:\n",
      "Tig IATI\n",
      "Tode, we?\n",
      "Larche:\n",
      "os lod:\n",
      "I\n",
      "Wo, blls.\n",
      "Tene\n",
      "f,\n",
      "Te;\n",
      "Tre,,\n",
      "I:\n",
      "TAgf LETlin:\n",
      "TIs,\n",
      "weae.\n",
      "WLTlls,\n",
      "\n",
      "====================================\n",
      "step: 40000 | average training loss: 2.5105 | average test loss: 2.5142\n",
      "Generating a sample: \n",
      "Verily my lord arowe-fde, t, ave, ke t\n",
      "G?\n",
      "Gede t,\n",
      "St 'd t\n",
      "Ted t.\n",
      "Gat't;\n",
      "Tus t;'t; hedAd\n",
      "H?\n",
      "G ist:\n",
      "Gad' f t\n",
      "O\n",
      "\n",
      "S alaglefakit, an:\n",
      "G his,\n",
      "GI\n",
      "\n",
      "mis;\n",
      "I\n",
      "G t:\n",
      "S\n",
      "Tss t bat.\n",
      "Taden,\n",
      "TE\n",
      "s,;\n",
      "hs?\n",
      "Ase, de h, s ne d l mared'sttlll\n",
      "====================================\n",
      "step: 50000 | average training loss: 2.5062 | average test loss: 2.5107\n",
      "Generating a sample: \n",
      "Verily my lord 'san pieieset h mesaus at vewit, ade mine Cas?nceruasensoeer be owpin-I esirithacin lins te s ce'ssoie, s eeaaverd sisst t'sorss, oue; s ans sasies aisshesouans acovsesr, atens hs, asass h, nicokasari\n",
      "====================================\n",
      "step: 60000 | average training loss: 2.4783 | average test loss: 2.4841\n",
      "Generating a sample: \n",
      "Verily my lord o, ieronerer our feerot?\n",
      "Aine e g.\n",
      "Se, ort t.\n",
      "Leerirdt ts!\n",
      "Nurered w t.\n",
      "Te;\n",
      "Nod ioue o tat y t ine! iou.\n",
      "Bid iony.\n",
      "Bs t.\n",
      "Wokeled len, imod!\n",
      "F t ou iT?\n",
      "Aro, t, t t te s?\n",
      "Lone Mif t?\n",
      "fhieliouner anoue d\n",
      "====================================\n",
      "step: 70000 | average training loss: 2.4844 | average test loss: 2.4882\n",
      "Generating a sample: \n",
      "Verily my lord awas. animans, h: wacanak h, re, mwhh--as, f'srs ags:\n",
      "T h Teaienracelen tan h s h, un h.\n",
      "qas, win:; crere:\n",
      "-S:\n",
      "i: rereran s Fer m,; faws:n: a: be, h cath. s: ilf:\n",
      "T:\n",
      "as h, h kaw uacaint pros: jin an:\n",
      "\n",
      "====================================\n",
      "step: 80000 | average training loss: 2.4743 | average test loss: 2.4798\n",
      "Generating a sample: \n",
      "Verily my lord faininemiore n:\n",
      "Hananery.\n",
      "d bind.\n",
      "d d acom d d, o anauinatingo: maaint fht gines.\n",
      "AAvead b.\n",
      "ds.\n",
      "Sinnand.\n",
      "Ay lysmoy y!st: naselinstins!\n",
      "Iy.\n",
      "obtacy.\n",
      "F:\n",
      "Aiet: dd s\n",
      "Aysly d taned::.\n",
      "d ine: ais Cades y wy \n",
      "====================================\n",
      "step: 90000 | average training loss: 2.5038 | average test loss: 2.5092\n",
      "Generating a sample: \n",
      "Verily my lord n t sowsotlowakaripinen,\n",
      "Canteco, cen on.\n",
      "Cacor, unsss Cs\n",
      "W mow!\n",
      "gereeran,\n",
      "LAR tstr n,\n",
      "aceclllllly tugen hepcerth, neaslo.\n",
      "LARaclacas atorececas tll,\n",
      "LAlebon f.\n",
      "\n",
      "\n",
      "N t on te.\n",
      "ALAnobteagecurall ctar, ti\n",
      "====================================\n",
      "step: 100000 | average training loss: 2.4776 | average test loss: 2.4823\n",
      "Generating a sample: \n",
      "Verily my lord my y atieest Ledotoy st st d my ny se ttt tge\n",
      "SstI tststlelerts iotstotssstetotholy the wenest I tecey s y may d'dedsme nowetststowe omeepery tet mely isthasteeent t my moweessteatwey hee lerstidenist\n",
      "====================================\n",
      "step: 110000 | average training loss: 2.4767 | average test loss: 2.4816\n",
      "Generating a sample: \n",
      "Verily my lord oo cerycinouy ie aty, he o t cond ndy he cere t, outis Be ry tpe tery, tostche shes o pe s ture cod, dndest lo;\n",
      "Tocinooeysoresponf's sansspeshe tpendstd aftte wh, cosinduess chiery wiach IOs hy be fpe\n",
      "====================================\n",
      "step: 120000 | average training loss: 2.4768 | average test loss: 2.4819\n",
      "Generating a sample: \n",
      "Verily my lord henth ETemerormoueis s.'d\n",
      "fo'ds ads,\n",
      "s\n",
      "Tto\n",
      "\n",
      "Tht t,\n",
      "ETt t h\n",
      "He\n",
      "I w\n",
      "Were.\n",
      "\n",
      "aveerest,\n",
      "Ton\n",
      "y y htee ain'sst fose somame\n",
      "ILy sth,\n",
      "H s,\n",
      "Hen foute us mes s are\n",
      "fat'rero'd,\n",
      "y s ay ooLhe,\n",
      "Are tisered une, Hese\n",
      "====================================\n",
      "step: 130000 | average training loss: 2.4822 | average test loss: 2.4885\n",
      "Generating a sample: \n",
      "Verily my lord horirou,\n",
      "l atlowonliy serld Euslourotourys,\n",
      "T tlansperely coTst,\n",
      "E\n",
      "daghy\n",
      "T:\n",
      "E cans, likak\n",
      "Hss y,\n",
      "Evengepag mounerure\n",
      "I\n",
      "IN,\n",
      "Huromus;\n",
      "S:\n",
      "Iese,\n",
      "Ts,\n",
      "I ng,\n",
      "E\n",
      "per ppertr,\n",
      "ECon INores y g nfepares trppactile\n",
      "====================================\n",
      "step: 140000 | average training loss: 2.5053 | average test loss: 2.5102\n",
      "Generating a sample: \n",
      "Verily my lord uras y,\n",
      "Y\n",
      "Ms ded,\n",
      "Hine,\n",
      "Lnris ue wimsstclllleres!\n",
      "ch t gopo,\n",
      "R fedcav g,\n",
      "Anee'ssthslys,\n",
      "fy h nclys,\n",
      "Amy,\n",
      "ag gaves t mere s clenocesw'spes;\n",
      "Teneralances,st s,\n",
      "Terers cks, st,\n",
      "bclee ayuly vnelroce lerow\n",
      "====================================\n",
      "step: 150000 | average training loss: 2.4790 | average test loss: 2.4823\n",
      "Generating a sample: \n",
      "Verily my lord hes ist hes higind; s cred h.\n",
      "sere anct tsfereliniveshfe utartsine,\n",
      "Mise, st\n",
      "I t wed\n",
      "I--rinsrese hes I\n",
      "I d hedirersd tined aner'paners o ariligt\n",
      "Ke.\n",
      "Kainediere h wy tacerile\n",
      "Arere d\n",
      "I\n",
      "I---aest baid hi\n",
      "====================================\n",
      "step: 160000 | average training loss: 2.4458 | average test loss: 2.4517\n",
      "Generating a sample: \n",
      "Verily my lord t tno, t py.\n",
      "Anale.\n",
      "Hiatepad pay tuinuno acLed riend t wercd tlave t: pled ceravel lealed be ourilir ly, t tr w t br t weme os w LLLeder,\n",
      "wesh le lind thiedd, Ii, ce, t.\n",
      "Y be te, t any;\n",
      "D teled t tte,\n",
      "====================================\n",
      "step: 170000 | average training loss: 2.4354 | average test loss: 2.4422\n",
      "Generating a sample: \n",
      "Verily my lord cerveerapowi-d t, d O t rerered yce, hes at n winow g merered'd;\n",
      "Med; femeaknt aruerneg s eat ugeroameilivamare-ve ay t wero reledm's.Py.\n",
      "Trt K n--ared t ent hy d te ch:\n",
      "Waren to acowhy kean puren:\n",
      "Th\n",
      "====================================\n",
      "step: 180000 | average training loss: 2.4830 | average test loss: 2.4862\n",
      "Generating a sample: \n",
      "Verily my lord fy wain-wanen, xwary Ins banreno;; amai'd--f famay--dsy y.\n",
      "Hmont marant be t marame figs may mt t; ty y may fe, pomamam.\n",
      "t bay tain;'fay, abak man f bat, mamatrar;\n",
      "Thas, y.\n",
      "Haknary, math.\n",
      "feAo-t minef\n",
      "====================================\n",
      "step: 190000 | average training loss: 2.4252 | average test loss: 2.4308\n",
      "Generating a sample: \n",
      "Verily my lord moranity boonoanenoronerinocoporersf ns! lortheeolomopot waveromowororveeoperronooooory,\n",
      "Arerv:\n",
      "poropnorsorine, vespotaoronovsono, wineoruronareponovere, oroivenororo,\n",
      "Borinensonon loronveoriomont ove\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "## Works with no issues ##\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = 8 # length of the input sequences of integers\n",
    "    vocab_size: int = vocab_size # the input integers are in range [0 .. vocab_size -1]\n",
    "    # parameters below control the sizes of each model slightly differently\n",
    "    # n_layer: int = 1\n",
    "    n_embd: int = 10\n",
    "    n_embd2: int = 100\n",
    "    # n_head: int = 4\n",
    "    \n",
    "class RNNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    the job of a 'Cell' is to:\n",
    "    take input at current time step x_{t} and the hidden state at the\n",
    "    previous time step h_{t-1} and return the resulting hidden state\n",
    "    h_{t} at the current timestep\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.xh_to_h = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "\n",
    "    def forward(self, xt, hprev):\n",
    "        xh = torch.cat([xt, hprev], dim=1)\n",
    "        ht = F.tanh(self.xh_to_h(xh))\n",
    "        return ht\n",
    "        \n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, config, cell_type):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.start = nn.Parameter(torch.zeros(1, config.n_embd2)) # the starting hidden state\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # token embeddings table\n",
    "        if cell_type == 'rnn':\n",
    "            self.cell = RNNCell(config)\n",
    "        elif cell_type == 'gru':\n",
    "            self.cell = GRUCell(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd2, self.vocab_size)\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "\n",
    "        # embed all the integers up front and all at once for efficiency\n",
    "        emb = self.wte(idx) # (b, t, n_embd)\n",
    "\n",
    "        # sequentially iterate over the inputs and update the RNN state each tick\n",
    "        # hprev = self.start.expand((b, -1)) # expand out the batch dimension\n",
    "        ht = self.start.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        for i in range(t):\n",
    "            xt = emb[:, i, :] # (b, n_embd)\n",
    "            ht = self.cell(xt, ht) # (b, n_embd2)\n",
    "            # hprev = ht\n",
    "            hiddens.append(ht)\n",
    "\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return loss, logits\n",
    "num_iters = 200_000\n",
    "m = ModelConfig()\n",
    "\n",
    "rnn = RNN(m, 'rnn')\n",
    "# init optimizer\n",
    "optim = torch.optim.AdamW(rnn.parameters(), lr=0.01, betas=(0.9, 0.99), eps=1e-8)\n",
    "\n",
    "for i in range(num_iters):\n",
    "    x, y = get_batch(train_data)\n",
    "    loss, _ = rnn(x, y)\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    if not (i % 10000):\n",
    "        print(f'step: {i}', end=' | ')\n",
    "        evaluate(rnn)\n",
    "        # print(f'iter: {i}: train loss: {loss:.4f}')\n",
    "        print('Generating a sample: ')\n",
    "        generate(\"Verily my lord \", rnn, max_output_size=200)\n",
    "        print('\\n====================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70956fb-0002-42b9-90db-23093d2b8ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
